{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3756c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dca8b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load env variables\n",
    "load_dotenv('../../../.env')\n",
    "\n",
    "# Import your model and data loading components\n",
    "from dataloader.dataset_wrapper import create_wrapper_from_dataframe\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2e974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=d961f7865a744c788142c5e566900c75\n",
      "ClearML results page: https://app.clear.ml/projects/0fec81950d384f0294d2c713df3887db/experiments/d961f7865a744c788142c5e566900c75/output/log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n"
     ]
    }
   ],
   "source": [
    "# First install ClearML if not already installed: pip install clearml\n",
    "import clearml\n",
    "from clearml import Task, Logger\n",
    "\n",
    "# Initialize ClearML Task\n",
    "task = Task.init(project_name='CSMP_thesis_project', task_name='CSMP_traning_experiment_3', reuse_last_task_id=False)\n",
    "logger = Logger.current_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44018e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CUDA devices:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable CUDA devices:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clear cache to get accurate memory info\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     total_memory \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_properties(i)\u001b[38;5;241m.\u001b[39mtotal_memory\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/cuda/__init__.py:420\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    418\u001b[0m device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_setDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration and paths setup\n",
    "CONFIG_PATH = \"../../../configs/config.yaml\"\n",
    "TRAIN_CSV_PATH = \"../../../data/traning_and_validation/train_deduplicated.csv\"\n",
    "\n",
    "# Device selection with automatic CUDA device selection based on free memory\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    print(\"Using MPS (Metal Performance Shaders) for GPU acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    # Find CUDA device with most free memory\n",
    "    max_free_memory = 0\n",
    "    best_device = 0\n",
    "    \n",
    "    print(\"Available CUDA devices:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        torch.cuda.set_device(i)\n",
    "        torch.cuda.empty_cache()  # Clear cache to get accurate memory info\n",
    "        \n",
    "        total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(i)\n",
    "        free_memory = total_memory - allocated_memory\n",
    "        \n",
    "        print(f\"  Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Total memory: {total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    Allocated memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"    Free memory: {free_memory / 1024**3:.2f} GB\")\n",
    "        \n",
    "        if free_memory > max_free_memory:\n",
    "            max_free_memory = free_memory\n",
    "            best_device = i\n",
    "    \n",
    "    DEVICE = f'cuda:{best_device}'\n",
    "    print(f\"\\nSelected device {best_device} with {max_free_memory / 1024**3:.2f} GB free memory\")\n",
    "    print(\"Using CUDA for GPU acceleration\")\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Validation data path: {TRAIN_CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9502b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration...\n",
      "Configuration loaded:\n",
      "- Batch size: 256\n",
      "- Model config keys: []\n",
      "- Loss config: {'temperature': 0.1, 'use_cosine_similarity': True, 'alpha_weight': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load configuration\n",
    "print(\"Loading configuration...\")\n",
    "config = yaml.load(open(CONFIG_PATH, \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Batch size: {config.get('batch_size', 64)}\")\n",
    "print(f\"- Model config keys: {list(config.get('model', {}).keys())}\")\n",
    "print(f\"- Loss config: {config.get('loss', {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e06fc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1024,\n",
       " 'epochs': 100,\n",
       " 'eval_every_n_epochs': 5,\n",
       " 'log_every_n_steps': 10,\n",
       " 'learning_rate': '1e-05',\n",
       " 'weight_decay': 0.0001,\n",
       " 'valid_size': 0.2,\n",
       " 'fp16_precision': True,\n",
       " 'truncation': True,\n",
       " 'model_config': {'emb_dim': 512,\n",
       "  'spec_embed_dim': 512,\n",
       "  'embed_dim': 256,\n",
       "  'feat_dim': 512,\n",
       "  'num_layer': 5,\n",
       "  'layers': 5,\n",
       "  'drop_ratio': 0.2,\n",
       "  'dropout': 0.1,\n",
       "  'pool': 'mean'},\n",
       " 'loss': {'temperature': 0.1,\n",
       "  'use_cosine_similarity': True,\n",
       "  'alpha_weight': 0.75}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.connect(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfbadd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "Validation dataset shape: (798444, 10)\n",
      "Columns: ['peaks_json', 'ion_source', 'compound_source', 'instrument', 'adduct', 'precursor_mz', 'smiles', 'inchikey', 'ion_mode', 'molecular_formula']\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peaks_json</th>\n",
       "      <th>ion_source</th>\n",
       "      <th>compound_source</th>\n",
       "      <th>instrument</th>\n",
       "      <th>adduct</th>\n",
       "      <th>precursor_mz</th>\n",
       "      <th>smiles</th>\n",
       "      <th>inchikey</th>\n",
       "      <th>ion_mode</th>\n",
       "      <th>molecular_formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[42.014248, 0.10199999999999998], [42.26601, ...</td>\n",
       "      <td>ESI</td>\n",
       "      <td>Crude</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>377.186</td>\n",
       "      <td>CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...</td>\n",
       "      <td>RNKMIWQDRWSWCD-UHFFFAOYSA-N</td>\n",
       "      <td>Positive</td>\n",
       "      <td>C23H24N2O3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[49.01717, 0.155], [49.020023, 0.253], [67.05...</td>\n",
       "      <td>ESI</td>\n",
       "      <td>Crude</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>377.186</td>\n",
       "      <td>CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...</td>\n",
       "      <td>RNKMIWQDRWSWCD-UHFFFAOYSA-N</td>\n",
       "      <td>Positive</td>\n",
       "      <td>C23H24N2O3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[49.017338, 0.242], [49.020237, 0.181], [67.0...</td>\n",
       "      <td>ESI</td>\n",
       "      <td>Crude</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>377.186</td>\n",
       "      <td>CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...</td>\n",
       "      <td>RNKMIWQDRWSWCD-UHFFFAOYSA-N</td>\n",
       "      <td>Positive</td>\n",
       "      <td>C23H24N2O3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[49.01701, 0.144], [49.019947, 0.244], [139.0...</td>\n",
       "      <td>ESI</td>\n",
       "      <td>Crude</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>377.186</td>\n",
       "      <td>CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...</td>\n",
       "      <td>RNKMIWQDRWSWCD-UHFFFAOYSA-N</td>\n",
       "      <td>Positive</td>\n",
       "      <td>C23H24N2O3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[49.017166, 0.155], [49.020008, 0.253], [139....</td>\n",
       "      <td>ESI</td>\n",
       "      <td>Crude</td>\n",
       "      <td>Orbitrap</td>\n",
       "      <td>[M+H]+</td>\n",
       "      <td>377.186</td>\n",
       "      <td>CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...</td>\n",
       "      <td>RNKMIWQDRWSWCD-UHFFFAOYSA-N</td>\n",
       "      <td>Positive</td>\n",
       "      <td>C23H24N2O3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          peaks_json ion_source  \\\n",
       "0  [[42.014248, 0.10199999999999998], [42.26601, ...        ESI   \n",
       "1  [[49.01717, 0.155], [49.020023, 0.253], [67.05...        ESI   \n",
       "2  [[49.017338, 0.242], [49.020237, 0.181], [67.0...        ESI   \n",
       "3  [[49.01701, 0.144], [49.019947, 0.244], [139.0...        ESI   \n",
       "4  [[49.017166, 0.155], [49.020008, 0.253], [139....        ESI   \n",
       "\n",
       "  compound_source instrument  adduct  precursor_mz  \\\n",
       "0           Crude   Orbitrap  [M+H]+       377.186   \n",
       "1           Crude   Orbitrap  [M+H]+       377.186   \n",
       "2           Crude   Orbitrap  [M+H]+       377.186   \n",
       "3           Crude   Orbitrap  [M+H]+       377.186   \n",
       "4           Crude   Orbitrap  [M+H]+       377.186   \n",
       "\n",
       "                                              smiles  \\\n",
       "0  CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...   \n",
       "1  CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...   \n",
       "2  CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...   \n",
       "3  CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...   \n",
       "4  CC12CCC(C(=O)N(CNc3cc4c(cc3)c3ccccc3o4)C1=O)C2...   \n",
       "\n",
       "                      inchikey  ion_mode molecular_formula  \n",
       "0  RNKMIWQDRWSWCD-UHFFFAOYSA-N  Positive        C23H24N2O3  \n",
       "1  RNKMIWQDRWSWCD-UHFFFAOYSA-N  Positive        C23H24N2O3  \n",
       "2  RNKMIWQDRWSWCD-UHFFFAOYSA-N  Positive        C23H24N2O3  \n",
       "3  RNKMIWQDRWSWCD-UHFFFAOYSA-N  Positive        C23H24N2O3  \n",
       "4  RNKMIWQDRWSWCD-UHFFFAOYSA-N  Positive        C23H24N2O3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load and explore validation data\n",
    "print(\"Loading train data...\")\n",
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "print(f\"Validation dataset shape: {df_train.shape}\")\n",
    "print(f\"Columns: {list(df_train.columns)}\")\n",
    "print(f\"Sample data:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da40ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sample = df_train.sample(n=10000,random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed921164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data loaders\n",
      "Converting DataFrame to compatible files...\n",
      "Processed 10000 valid spectra out of 10000 total entries.\n",
      "Create data wrapper\n",
      "calculating molecular graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 453/8000 [00:00<00:12, 624.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [I-].O=C(OCC1=CC[N+]2(C)CCC(O)C12)C(O)(C(O)C)C(C)C calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 845/8000 [00:01<00:11, 650.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Cl-].O=C(O)C=1C=CC=CC1C=2C=3C=CC(=CC3OC4=CC(C=CC42)=[N+](CC)CC)N(CC)CC calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1363/8000 [00:02<00:10, 641.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Na+].O=C(CCCCCCCCCCC)CC(O)S(=O)(=O)[O-] calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1684/8000 [00:02<00:10, 627.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Cl-].O=C1C(=COC2=C1C=C(C(O)=C2C[NH+](C)C)CC)C=3C=CC=4OCCOC4C3 calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2272/8000 [00:03<00:08, 646.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Cl-].O=C1C=2C=C(C(O)=C(C2OC(=C1C=3C=CC=4OCCOC4C3)C)C[NH+](C)C)CCC calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 2985/8000 [00:04<00:08, 626.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Cl-].O=C1C2=CC=C(O)C(=C2OC(=C1C=3C=CC=4OCCCOC4C3)C)C[NH+](C)C calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 3178/8000 [00:05<00:07, 630.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Na+].O=P([O-])(O)OCC1OC(N2C=NC=3C(=NC=NC32)N)C(O)C1O calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 3624/8000 [00:05<00:06, 632.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Cl-].O=C1C2=CC=C(O)C(=C2OC(=C1C=3C=CC=4OCCCOC4C3)C)C[NH+](C)C calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 4138/8000 [00:06<00:06, 628.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [I-].O=C(OCC1=CC[N+]2(C)CCC(O)C12)C(O)(C(O)C)C(C)C calculation failure\n",
      "SMILES [Cl-].OC=1C=C(O)C=2C=C(OC3OC(CO)C(O)C(O)C3O)C(=[O+]C2C1)C=4C=CC(O)=C(O)C4 calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4590/8000 [00:07<00:05, 627.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Na+].O=C(CCCCCCCCCCC)CC(O)S(=O)(=O)[O-] calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 4974/8000 [00:07<00:04, 626.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES CCC1=C(C2=NC1=CC3=C(C4=C([N-]3)C(=C5[C@H]([C@@H](C(=N5)C=C6C(=C(C(=C2)[N-]6)C=C)C)C)CCC(=O)OC/C=C(\\C)/CCC[C@H](C)CCC[C@H](C)CCCC(C)C)[C@H](C4=O)C(=O)OC)C)C=O.[Mg+2] calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 5163/8000 [00:08<00:04, 619.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [I-].O=C(OCC1=CC[N+]2(C)CCC(O)C12)C(O)(C(O)C)C(C)C calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 5603/8000 [00:08<00:03, 615.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES C1C(N(C2=C(N1)N=C(NC2=O)N)C=O)CNC3=CC=C(C=C3)C(=O)N[C@@H](CCC(=O)[O-])C(=O)[O-].[Ca+2] calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 6112/8000 [00:09<00:03, 616.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Na+].O=C([O-])C(CC)C1OC(C(=CC=CC2C=CC3CCCC3C2C(=O)C4=CC=CN4)CC)C(C)CC1 calculation failure\n",
      "SMILES [K+].[K+].O=C([O-])C1OC(OC2C(OC(C(=O)[O-])C(O)C2O)OC3CCC4(C)C5C(=O)C=C6C7CC(C(=O)O)(C)CCC7(C)CCC6(C)C5(C)CCC4C3(C)C)C(O)C(O)C1O calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 7137/8000 [00:11<00:01, 627.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [K+].O=C([O-])C12CCC(C(=C)C)C2C3CCC4C5(C)CCC(=O)C(C)(C)C5CCC4(C)C3(C)CC1 calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:12<00:00, 628.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 6875 molecular graph-mass spectrometry pairs\n",
      "calculating molecular graphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 67/2000 [00:00<00:02, 665.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Na+].O=C(CCCCCCCCCCC)CC(O)S(=O)(=O)[O-] calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 329/2000 [00:00<00:02, 632.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Br-].O=C(OC1CC2C3OC3C(C1)[N+]2(C)CCCC)C(C=4C=CC=CC4)CO calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 658/2000 [00:01<00:02, 641.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [I-].O=C(OCC1=CC[N+]2(C)CCC(O)C12)C(O)(C(O)C)C(C)C calculation failure\n",
      "SMILES [K+].O=S(=O)([O-])ON=C(SC1OC(CO)C(O)C(O)C1O)CC=C.O calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 859/2000 [00:01<00:01, 648.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Cl-].OC=1C=C(O)C=2C=C(OC3OC(CO)C(O)C(O)C3O)C(=[O+]C2C1)C=4C=CC(O)=C(O)C4 calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 1383/2000 [00:02<00:00, 619.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES [Cl-].O=C1C2=CC=C(O)C(=C2OC(=C1C=3C=CC=4OCCCOC4C3)C)C[NH+](C)C calculation failure\n",
      "SMILES [K+].O=S(=O)([O-])ON=C(SC1OC(CO)C(O)C(O)C1O)CC=C.O calculation failure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:03<00:00, 636.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated 1697 molecular graph-mass spectrometry pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Prepare validation data loader\n",
    "print(\"Preparing data loaders\")\n",
    "\n",
    "# Create data wrapper from DataFrame\n",
    "wrapper = create_wrapper_from_dataframe(\n",
    "    df=df_train_sample,\n",
    "    batch_size=config.get('batch_size'),  \n",
    "    num_workers=8,\n",
    "    valid_size=config.get('valid_size'),  \n",
    "    use_ddp=False,\n",
    "    output_dir=\"../../../data/train_feature/\",\n",
    "    recompute=True\n",
    ")\n",
    "\n",
    "# Get the data loader\n",
    "train_loader, val_loader = wrapper.get_data_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d777a5a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCLR\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize model architecture\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelCLR\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from model import ModelCLR\n",
    "\n",
    "# Initialize model architecture\n",
    "model = ModelCLR(**config[\"model_config\"]).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc0c56bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelCLR(\n",
       "  (Smiles_model): SmilesModel(\n",
       "    (x_embedding1): Embedding(119, 512)\n",
       "    (x_embedding2): Embedding(4, 512)\n",
       "    (x_embedding3): Embedding(8, 512)\n",
       "    (x_embedding4): Embedding(6, 512)\n",
       "    (x_embedding5): Embedding(5, 512)\n",
       "    (gnns): ModuleList(\n",
       "      (0-4): 5 x GINEConv()\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-4): 5 x BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (feat_lin): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (out_lin): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (MS_model): MSModel(\n",
       "    (mz_embedder): FourierEmbedder()\n",
       "    (input_compress): Linear(in_features=513, out_features=512, bias=True)\n",
       "    (peak_attn_layers): ModuleList(\n",
       "      (0-4): 5 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (pooling_layer): AdaptiveAvgPool1d(output_size=1)\n",
       "    (output_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (smi_esa): ESA_SMILES(\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (spec_esa): ESA_SPEC(\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (smi_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (spec_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eab11f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER ANALYSIS:\n",
      "----------------------------------------\n",
      "Trainable Parameters: 22,821,120\n"
     ]
    }
   ],
   "source": [
    "# Parameter Count\n",
    "print(\"PARAMETER ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e1bd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEMORY ANALYSIS:\n",
      "----------------------------------------\n",
      "Model Size: 87.08 MB\n",
      "Parameter Memory: 87.06 MB\n",
      "Buffer Memory: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "# Memory Usage (approximate)\n",
    "print(\"MEMORY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "model_size_mb = (param_size + buffer_size) / 1024 / 1024\n",
    "\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "print(f\"Parameter Memory: {param_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Buffer Memory: {buffer_size / 1024 / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9e6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss.nt_xent import NTXentLoss\n",
    "\n",
    "# Initialize loss function\n",
    "temperature = config.get('loss', {}).get('temperature', 0.1)\n",
    "batch_size = config.get('batch_size', 512)\n",
    "use_cosine_similarity = config.get('loss', {}).get('use_cosine_similarity', True)\n",
    "alpha_weight = config.get('loss', {}).get('alpha_weight', 1.0)\n",
    "\n",
    "criterion = NTXentLoss(\n",
    "    device=DEVICE, \n",
    "    batch_size=batch_size, \n",
    "    temperature=temperature, \n",
    "    use_cosine_similarity=use_cosine_similarity, \n",
    "    alpha_weight=alpha_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca14bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training components...\n",
      "Training for 100 epochs\n",
      "Optimizer: AdamW with lr=1e-05\n",
      "Checkpoint directory: ../../../models/models_experiments/candidate_v1/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Training Setup and Optimizer\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "print(\"Setting up training components...\")\n",
    "OUTPUT_DIR = \"../../../models/models_experiments/candidate_v1\"\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=float(config.get('learning_rate', 1e-4)),\n",
    "    weight_decay=float(config.get('weight_decay', 1e-4))\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "epochs = config.get('epochs', 100)\n",
    "eval_every_n_epochs = config.get('eval_every_n_epochs', 5)\n",
    "log_every_n_steps = config.get('log_every_n_steps', 2)\n",
    "\n",
    "# Add ReduceLROnPlateau scheduler for representation learning\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',           # minimize validation loss\n",
    "    factor=0.5,           # reduce LR by half\n",
    "    patience=3,             \n",
    "    verbose=True,\n",
    "    min_lr=1e-6,\n",
    "    threshold=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "# Create checkpoint directory\n",
    "checkpoint_dir = os.path.join(OUTPUT_DIR, \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Training for {epochs} epochs\")\n",
    "print(f\"Optimizer: AdamW with lr={config.get('learning_rate', 5e-6)}\")\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2270602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Training and Evaluation Functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    batch_losses = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, (graphs, mzs, intensities, num_peaks) in enumerate(progress_bar):\n",
    "        \n",
    "        # Move data to device\n",
    "        graphs = graphs.to(device)\n",
    "        mzs = mzs.to(device)\n",
    "        intensities = intensities.to(device)\n",
    "        num_peaks = num_peaks.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        mol_features, spec_features = model(graphs, mzs, intensities, num_peaks)\n",
    "        loss = criterion(mol_features, spec_features)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        batch_loss = loss.item()\n",
    "        total_loss += batch_loss\n",
    "        batch_losses.append(batch_loss)\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{batch_loss:.4f}',\n",
    "            'Avg Loss': f'{total_loss/num_batches:.4f}'\n",
    "        })\n",
    "        \n",
    "        # Log every n steps\n",
    "        if batch_idx % log_every_n_steps == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {batch_loss:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss, batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dadfa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    molecular_features_list = []\n",
    "    spectral_features_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc=f\"Evaluating Epoch {epoch}\")\n",
    "        \n",
    "        for batch_idx, (graphs, mzs, intensities, num_peaks) in enumerate(progress_bar):\n",
    "            # Move data to device\n",
    "            graphs = graphs.to(device)\n",
    "            mzs = mzs.to(device)\n",
    "            intensities = intensities.to(device)\n",
    "            num_peaks = num_peaks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            mol_features, spec_features = model(graphs, mzs, intensities, num_peaks)\n",
    "            loss = criterion(mol_features, spec_features)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Normalize features before storing \n",
    "            mol_features_norm = torch.nn.functional.normalize(mol_features, p=2, dim=1)\n",
    "            spec_features_norm = torch.nn.functional.normalize(spec_features, p=2, dim=1)\n",
    "            \n",
    "            # Store normalized features for retrieval metrics\n",
    "            molecular_features_list.append(mol_features_norm.cpu().numpy())\n",
    "            spectral_features_list.append(spec_features_norm.cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'Val Loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    # Compute retrieval metrics\n",
    "    all_mol_features = np.vstack(molecular_features_list)\n",
    "    all_spec_features = np.vstack(spectral_features_list)\n",
    "        \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = np.sum(all_mol_features * all_spec_features, axis=1)\n",
    "    mean_similarity = np.mean(cosine_similarities)\n",
    "        \n",
    "    return avg_loss, mean_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df13de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, checkpoint_dir):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Save best model\n",
    "    best_checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "    if not os.path.exists(best_checkpoint_path):\n",
    "        torch.save(checkpoint, best_checkpoint_path)\n",
    "        print(f\"Best model saved: {best_checkpoint_path}\")\n",
    "    else:\n",
    "        best_checkpoint = torch.load(best_checkpoint_path)\n",
    "        if val_loss < best_checkpoint['val_loss']:\n",
    "            torch.save(checkpoint, best_checkpoint_path)\n",
    "            print(f\"New best model saved: {best_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc7034d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/6 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.34 GiB. GPU 1 has a total capacity of 79.25 GiB of which 1.79 GiB is free. Process 3080882 has 3.27 GiB memory in use. Process 3128536 has 2.51 GiB memory in use. Process 3206825 has 414.00 MiB memory in use. Process 3207471 has 414.00 MiB memory in use. Including non-PyTorch memory, this process has 70.85 GiB memory in use. Of the allocated memory 66.58 GiB is allocated by PyTorch, and 3.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m train_loss, batch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get current learning rate\u001b[39;00m\n\u001b[1;32m     27\u001b[0m current_lr \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m mol_features, spec_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmzs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_peaks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(mol_features, spec_features)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/csmp_search_engine_for_specmol/CSMP_thesis_project/services/CSU-MS2/CSUMS2/model.py:278\u001b[0m, in \u001b[0;36mModelCLR.forward\u001b[0;34m(self, xis, mzs, intens, num_peaks)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xis, mzs,intens,num_peaks):\n\u001b[1;32m    277\u001b[0m     zis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmiles_encoder(xis)\n\u001b[0;32m--> 278\u001b[0m     zls,attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mms_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmzs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mintens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_peaks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     zis_feat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmi_esa(zis,xis\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m    280\u001b[0m     zls_feat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec_esa(zls,attn_mask)\n",
      "File \u001b[0;32m~/csmp_search_engine_for_specmol/CSMP_thesis_project/services/CSU-MS2/CSUMS2/model.py:273\u001b[0m, in \u001b[0;36mModelCLR.ms_encoder\u001b[0;34m(self, mzs, intens, num_peaks)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mms_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, mzs,intens,num_peaks):\n\u001b[0;32m--> 273\u001b[0m     out_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMS_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmzs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mintens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_peaks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_emb\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/csmp_search_engine_for_specmol/CSMP_thesis_project/services/CSU-MS2/CSUMS2/model.py:196\u001b[0m, in \u001b[0;36mMSModel.forward\u001b[0;34m(self, mzs, intens, num_peaks)\u001b[0m\n\u001b[1;32m    194\u001b[0m peak_tensor \u001b[38;5;241m=\u001b[39m peak_tensor\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m peak_attn_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeak_attn_layers:\n\u001b[0;32m--> 196\u001b[0m     peak_tensor, pairwise_features \u001b[38;5;241m=\u001b[39m \u001b[43mpeak_attn_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpeak_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m peak_tensor \u001b[38;5;241m=\u001b[39m peak_tensor\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Get only the class token\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m#h0 = peak_tensor[:, 0, :]\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m#output = self.output_layer(h0)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/csmp_search_engine_for_specmol/CSMP_thesis_project/services/CSU-MS2/CSUMS2/nn_utils/transformer_layers.py:127\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, pairwise_features, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    125\u001b[0m         x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, pairwise_features, src_key_padding_mask)\n\u001b[1;32m    126\u001b[0m     )\n\u001b[0;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, pairwise_features\n",
      "File \u001b[0;32m~/csmp_search_engine_for_specmol/CSMP_thesis_project/services/CSU-MS2/CSUMS2/nn_utils/transformer_layers.py:151\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 151\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CSU-MS2/lib/python3.8/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.34 GiB. GPU 1 has a total capacity of 79.25 GiB of which 1.79 GiB is free. Process 3080882 has 3.27 GiB memory in use. Process 3128536 has 2.51 GiB memory in use. Process 3206825 has 414.00 MiB memory in use. Process 3207471 has 414.00 MiB memory in use. Including non-PyTorch memory, this process has 70.85 GiB memory in use. Of the allocated memory 66.58 GiB is allocated by PyTorch, and 3.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training history for plotting\n",
    "train_history = {\n",
    "    'epochs': [],\n",
    "    'train_losses': [],\n",
    "    'val_losses': [],\n",
    "    'val_similarities': [],\n",
    "    'learning_rates': [],  \n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, batch_losses = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE, epoch\n",
    "    )\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    \n",
    "    # Log training loss every epoch\n",
    "    logger.report_scalar(\"Loss\", \"Train\", iteration=epoch, value=train_loss)\n",
    "    logger.report_scalar(\"Learning Rate\", \"LR\", iteration=epoch, value=current_lr)\n",
    "    \n",
    "    \n",
    "    # Validation phase (every n epochs)\n",
    "    if epoch % eval_every_n_epochs == 0:\n",
    "        val_loss, val_similarity = evaluate_model(\n",
    "            model, val_loader, criterion, DEVICE, epoch\n",
    "        )\n",
    "        \n",
    "        # Step scheduler with validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Update current_lr after scheduler step\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Log results\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Log validation metrics to ClearML\n",
    "        logger.report_scalar(\"Loss\", \"Validation\", iteration=epoch, value=val_loss)\n",
    "        logger.report_scalar(\"Similarity\", \"Cosine Similarity\", iteration=epoch, value=val_similarity)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.6f}\")\n",
    "        print(f\"Val Loss: {val_loss:.6f}\")\n",
    "        print(f\"Val mean similarity: {val_similarity:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "        print(f\"Epoch Time: {epoch_time:.2f}s, Total Time: {total_time:.2f}s\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Store history\n",
    "        train_history['epochs'].append(int(epoch))\n",
    "        train_history['train_losses'].append(float(train_loss))\n",
    "        train_history['val_losses'].append(float(val_loss))\n",
    "        train_history['val_similarities'].append(float(val_similarity))\n",
    "        train_history['learning_rates'].append(float(current_lr))\n",
    "        \n",
    "        # Save checkpoint\n",
    "        # save_checkpoint(\n",
    "        #     model, optimizer, epoch, \n",
    "        #     train_loss, val_loss, checkpoint_dir\n",
    "        # )\n",
    "        \n",
    "        if epoch % (eval_every_n_epochs * 2) == 0 and len(train_history['epochs']) > 1:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "            \n",
    "            # Loss plot\n",
    "            epochs_list = train_history['epochs']\n",
    "            ax1.plot(epochs_list, train_history['train_losses'], 'b-', label='Train Loss', linewidth=2)\n",
    "            ax1.plot(epochs_list, train_history['val_losses'], 'r-', label='Val Loss', linewidth=2)\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training and Validation Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Similarity plot\n",
    "            ax2.plot(epochs_list, train_history['val_similarities'], 'g-', linewidth=2)\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Cosine Similarity')\n",
    "            ax2.set_title('Validation Cosine Similarity')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Learning rate plot\n",
    "            ax3.plot(epochs_list, train_history['learning_rates'], 'purple', linewidth=2)\n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('Learning Rate')\n",
    "            ax3.set_title('Learning Rate Schedule')\n",
    "            ax3.set_yscale('log') \n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            logger.report_matplotlib_figure(\"Training Progress\", \"Loss, Similarity and LR\", iteration=epoch, figure=plt)\n",
    "            plt.close()\n",
    "    \n",
    "    else:\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.6f}, LR: {current_lr:.2e}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "# Final summary logging\n",
    "print(\"\\nTraining completed!\")\n",
    "total_training_time = (time.time() - start_time)/3600\n",
    "print(f\"Total training time: {total_training_time:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b84058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log final metrics\n",
    "logger.report_single_value(\"Total Training Time (hours)\", total_training_time)\n",
    "logger.report_single_value(\"Best Validation Loss\", min(train_history['val_losses']) if train_history['val_losses'] else float('inf'))\n",
    "logger.report_single_value(\"Best Cosine Similarity\", max(train_history['val_similarities']) if train_history['val_similarities'] else 0.0)\n",
    "logger.report_single_value(\"Total Epochs\", epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3d3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during cleanup: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "============================================================\n",
      "GPU MEMORY ANALYSIS\n",
      "============================================================\n",
      "CUDA available: 2 device(s)\n",
      "\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "----------------------------------------\n",
      "Total Memory:     79.25 GB\n",
      "Currently Allocated: 0.00 GB (0.0%)\n",
      "Currently Reserved:  0.00 GB (0.0%)\n",
      "Peak Allocated:   0.00 GB (0.0%)\n",
      "Peak Reserved:    0.00 GB (0.0%)\n",
      "Free Memory:      79.25 GB\n",
      "\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "----------------------------------------\n",
      "Total Memory:     79.25 GB\n",
      "Currently Allocated: 70.24 GB (88.6%)\n",
      "Currently Reserved:  72.71 GB (91.7%)\n",
      "Peak Allocated:   70.24 GB (88.6%)\n",
      "Peak Reserved:    72.71 GB (91.7%)\n",
      "Free Memory:      6.55 GB\n",
      "Memory Fragmentation: 3.4%\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Emergency GPU cleanup and memory monitoring\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "def emergency_gpu_cleanup():\n",
    "    \"\"\"Emergency cleanup to free all GPU memory\"\"\"\n",
    "    try:\n",
    "        # Clear all cached tensors\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Try to reset CUDA context (nuclear option)\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                print(\"CUDA memory stats reset\")\n",
    "            except:\n",
    "                print(\"Could not reset CUDA memory stats\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {e}\")\n",
    "\n",
    "def print_gpu_memory_usage():\n",
    "    \"\"\"Print comprehensive GPU memory usage\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GPU MEMORY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f\"CUDA available: {device_count} device(s)\")\n",
    "        \n",
    "        for i in range(device_count):\n",
    "            try:\n",
    "                torch.cuda.set_device(i)\n",
    "                device_name = torch.cuda.get_device_name(i)\n",
    "                \n",
    "                # Memory in bytes\n",
    "                allocated = torch.cuda.memory_allocated(i)\n",
    "                reserved = torch.cuda.memory_reserved(i)\n",
    "                max_allocated = torch.cuda.max_memory_allocated(i)\n",
    "                max_reserved = torch.cuda.max_memory_reserved(i)\n",
    "                \n",
    "                # Get total GPU memory\n",
    "                total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "                \n",
    "                print(f\"\\nGPU {i}: {device_name}\")\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"Total Memory:     {total_memory / 1024**3:.2f} GB\")\n",
    "                print(f\"Currently Allocated: {allocated / 1024**3:.2f} GB ({allocated/total_memory*100:.1f}%)\")\n",
    "                print(f\"Currently Reserved:  {reserved / 1024**3:.2f} GB ({reserved/total_memory*100:.1f}%)\")\n",
    "                print(f\"Peak Allocated:   {max_allocated / 1024**3:.2f} GB ({max_allocated/total_memory*100:.1f}%)\")\n",
    "                print(f\"Peak Reserved:    {max_reserved / 1024**3:.2f} GB ({max_reserved/total_memory*100:.1f}%)\")\n",
    "                print(f\"Free Memory:      {(total_memory - reserved) / 1024**3:.2f} GB\")\n",
    "                \n",
    "                # Memory fragmentation analysis\n",
    "                if reserved > 0:\n",
    "                    fragmentation = (reserved - allocated) / reserved * 100\n",
    "                    print(f\"Memory Fragmentation: {fragmentation:.1f}%\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading GPU {i} memory: {e}\")\n",
    "                \n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"MPS (Metal Performance Shaders) available\")\n",
    "        print(\"Note: MPS memory monitoring not directly available\")\n",
    "        \n",
    "        # Get system memory as proxy\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"System Memory: {mem.total / 1024**3:.2f} GB\")\n",
    "        print(f\"Available Memory: {mem.available / 1024**3:.2f} GB\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No GPU acceleration available - using CPU\")\n",
    "        \n",
    "        # Show CPU memory usage\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"System Memory: {mem.total / 1024**3:.2f} GB\")\n",
    "        print(f\"Available Memory: {mem.available / 1024**3:.2f} GB\")\n",
    "        print(f\"Used Memory: {mem.used / 1024**3:.2f} GB ({mem.percent:.1f}%)\")\n",
    "\n",
    "def print_model_memory_usage(model):\n",
    "    \"\"\"Calculate and print model memory usage\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL MEMORY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Parameter memory\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    \n",
    "    # Count parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"Model Parameters:\")\n",
    "    print(f\"  Trainable: {trainable_params:,}\")\n",
    "    print(f\"  Total: {total_params:,}\")\n",
    "    print(f\"  Non-trainable: {total_params - trainable_params:,}\")\n",
    "    \n",
    "    print(f\"\\nModel Memory:\")\n",
    "    print(f\"  Parameters: {param_size / 1024**2:.2f} MB\")\n",
    "    print(f\"  Buffers: {buffer_size / 1024**2:.2f} MB\")\n",
    "    print(f\"  Total Model: {(param_size + buffer_size) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Estimate training memory (rough approximation)\n",
    "    # Forward pass ≈ model size, backward pass ≈ 2x model size, optimizer ≈ 2x model size\n",
    "    estimated_training_memory = (param_size + buffer_size) * 5  # Conservative estimate\n",
    "    print(f\"  Estimated Training Memory: {estimated_training_memory / 1024**2:.2f} MB\")\n",
    "\n",
    "def monitor_batch_memory(device, batch_size):\n",
    "    \"\"\"Monitor memory usage during a training step\"\"\"\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        print(f\"\\nBatch Memory Analysis (Batch Size: {batch_size}):\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Memory before\n",
    "        mem_before = torch.cuda.memory_allocated() / 1024**2\n",
    "        \n",
    "        # Simulate memory usage estimation\n",
    "        print(f\"Memory before batch: {mem_before:.2f} MB\")\n",
    "        \n",
    "        return mem_before\n",
    "    return 0\n",
    "\n",
    "# Run emergency cleanup first\n",
    "emergency_gpu_cleanup()\n",
    "\n",
    "# Print initial memory status\n",
    "print_gpu_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the task\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSU-MS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
