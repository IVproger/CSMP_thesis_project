batch_size: 256
epochs: 100
eval_every_n_epochs: 5
# smilesmodel_finetune: True | False
# smilesmodel_finetune_path: path_to_model_checkpoint
# fine_tune_from: path_to_silico_data
log_every_n_steps: 10
learning_rate: 1e-05
weight_decay: 0.0001
valid_size: 0.2
fp16_precision: True
truncation: True

model_config:
  emb_dim: 256 # embeddings for smiles 
  spec_embed_dim: 256 # embeddings for spectrum
  embed_dim: 128 # projection embeddings
  
  feat_dim: 512 # size of feature for the smiles decoder
  num_layer: 5 # number of layers for SMILES encoder
  layers: 5 # number of layers for MS transformer 

  drop_ratio: 0.3  # For SMILES model
  dropout: 0.1 # For MS model

  pool : mean
  
loss:
  temperature: 0.1
  use_cosine_similarity: True
  alpha_weight: 0.75
